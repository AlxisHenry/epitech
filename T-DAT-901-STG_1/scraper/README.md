# ğŸ“˜ Scraper Service --- Documentation

## ğŸ“Œ PrÃ©sentation du service

Le service **Scraper** extrait automatiquement des donnÃ©es depuis le
site **CryptoCompare**.\
Il collecte :

- Les cryptomonnaies (`/all`)
- Les collections NFT (`/nft`)

Les donnÃ©es sont ensuite normalisÃ©es, packagÃ©es dans un objet `feed`,
puis envoyÃ©es vers Kafka (ou renvoyÃ©es en mode test).

Ce service utilise **Selenium** afin de charger et exÃ©cuter les pages
dynamiques fortement basÃ©es sur JavaScript.

---

## ğŸ§© Architecture du service

### **Structure gÃ©nÃ©rale**

    scraper/
     â”œâ”€â”€ src/
     â”‚    â”œâ”€â”€ main.py          # EntrÃ©e principale
     â”‚    â”œâ”€â”€ scraper.py       # Scraping Crypto / NFT
     â”‚    â”œâ”€â”€ tasks/
     â”‚    â”‚     â””â”€â”€ producer.py  # Envoi Kafka
     â”‚    â””â”€â”€ utils/ ...
     â”œâ”€â”€ README.md
     â””â”€â”€ requirements.txt

### **Fonctionnement du scraping**

1.  Ouvre Chrome headless avec Selenium\
2.  Charge les pages Crypto ou NFT\
3.  Attend le rendu dynamique (JavaScript)\
4.  Extrait les lignes du tableau\
5.  Construit un dictionnaire normalisÃ© `feed`\
6.  Envoie (ou non) le feed dans Kafka selon `TEST_MODE`

---

## âš™ï¸ Variables d'environnement

Dans le `.env` :

Variable Description

---

`TEST_MODE` Si `true`, n'envoie rien Ã  Kafka
`TOPIC_RAW` Nom du topic Kafka pour les donnÃ©es brutes

Exemple :

    TEST_MODE=true
    TOPIC_RAW=crypto.raw

---

## ğŸ› ï¸ Justification des technologies

### ğŸ Python

- Lisible, rapide Ã  dÃ©velopper\
- Excellent Ã©cosystÃ¨me pour scraping, automatisation, Kafka\
- Parfait pour un worker ou micro-service

### ğŸŒ Selenium

CryptoCompare utilise Angular â†’ contenu dynamique impossible Ã  scrapper
via `requests` + `BeautifulSoup`.

Selenium permet : - ExÃ©cution du JavaScript - Simulation utilisateur -
Extraction fiable des tables dynamiques

### ğŸ§° Chrome Headless

- Discret\
- Performant\
- Compatible CI/CD & Docker

### ğŸ§µ Kafka

- Pipeline ingestion â†’ traitement\
- RÃ©silient & scalable\
- DÃ©couplage total des services

---

## ğŸš€ Lancer le Scraper (en mode standalone)

Voici la procÃ©dure correcte pour **exÃ©cuter uniquement le scraper**,
mÃªme sans lancer Kafka.

### 1ï¸âƒ£ CrÃ©er un virtual environment

```bash
python3 -m venv .venv
```

### 2ï¸âƒ£ Activer le venv

#### Linux / MacOS :

```bash
source .venv/bin/activate
```

#### Windows :

```bash
.venv\Scripts\activate
```

### 3ï¸âƒ£ Installer les dÃ©pendances

Depuis la racine du projet :

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ ExÃ©cuter le scraper

Toujours **depuis la racine**, lancer :

```bash
python -m scraper.src.main
```

Ou :

```bash
python scraper/src/main.py
```

---

## ğŸ“¦ Structure du `feed` gÃ©nÃ©rÃ©

```json
{
  "timestamp": "2025-01-21 13:42:10",
  "crypto": [],
  "nft": []
}
```

---

## ğŸ§ª Mode Test

Quand `TEST_MODE=true` :

- Aucune donnÃ©e n'est envoyÃ©e vers Kafka\
- Le scraper renvoie directement le JSON\
- IdÃ©al pour dev, debug, et tests unitaires

---

## ğŸ“˜ Conclusion

Le service **Scraper** est la premiÃ¨re Ã©tape d'ingestion des donnÃ©es du
projet.\
GrÃ¢ce Ã  Python + Selenium + Kafka, il apporte :

- Un scraping fiable de pages dynamiques\
- Des donnÃ©es propres et centralisÃ©es\
- Une intÃ©gration facile avec le reste de l'architecture
